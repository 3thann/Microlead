Ici, nous allons d√©couvrir un algorithme d'optimisation dans Sklearn, appel√© descente de gradient stochastique (SGD - Stochastic Gradient Descent).

La descente de gradient stochastique (SGD) est un algorithme d'optimisation simple mais efficace utilis√© pour trouver les valeurs des param√®tres/coefficients des fonctions qui minimisent une fonction de co√ªt. En d'autres termes, il est utilis√© pour l'apprentissage discriminant de classificateurs lin√©aires sous des fonctions de perte convexes, comme les SVM et la r√©gression logistique. Il a √©t√© appliqu√© avec succ√®s √† des ensembles de donn√©es √† grande √©chelle car la mise √† jour des coefficients est effectu√©e pour chaque instance d'apprentissage, plut√¥t qu'√† la fin des instances.

## Classificateur SGD

Le classificateur SGD (Stochastic Gradient Descent) impl√©mente essentiellement une routine d'apprentissage SGD ordinaire prenant en charge diverses fonctions de perte et p√©nalit√©s pour la classification. Scikit-learn fournit le module SGDClassifier pour impl√©menter la classification SGD.

### Param√®tres

La liste suivante pr√©sente les param√®tres utilis√©s par le module SGDClassifier :

- **loss** - str, default = 'hinge' (perte). Il repr√©sente la fonction de perte √† utiliser lors de la mise en ≈ìuvre. La valeur par d√©faut est 'hinge' qui nous donnera un SVM lin√©aire. Les autres options qui peuvent √™tre utilis√©es sont :
    - **log** - Cette perte nous donnera une r√©gression logistique, c'est-√†-dire un classificateur probabiliste.
    - **modified_huber** - une perte lisse qui apporte une tol√©rance aux valeurs aberrantes ainsi que des estimations de probabilit√©.
    - **squared_hinge** - similaire √† la perte 'hinge' mais elle est p√©nalis√©e de mani√®re quadratique.
    - **perceptron** - comme son nom l'indique, il s'agit d'une perte lin√©aire qui est utilis√©e par l'algorithme perceptron.
- **penalty** - str, 'none', 'l2', 'l1', 'elasticnet'. Il s'agit du terme de r√©gularisation utilis√© dans le mod√®le. Par d√©faut, il s'agit de L2. Nous pouvons √©galement utiliser L1 ou 'elasticnet', mais ces deux termes risquent d'apporter de la sparsit√© au mod√®le, ce qui n'est pas r√©alisable avec L2.
- **alpha** - flottant, par d√©faut = 0,0001. Alpha, la constante qui multiplie le terme de r√©gularisation, est le param√®tre d'accord qui d√©cide dans quelle mesure nous voulons p√©naliser le mod√®le. La valeur par d√©faut est 0.0001.
- **l1_ratio** - float, default = 0.15. Il s'agit du param√®tre de m√©lange ElasticNet. Sa plage est 0 < = l1_ratio < = 1. Si l1_ratio = 1, la p√©nalit√© serait la p√©nalit√© L1. Si l1_ratio = 0, la p√©nalit√© serait une p√©nalit√© L2.
- **fit_intercept** - Bool√©en, D√©faut=True. Ce param√®tre indique si une constante (biais ou intercept) doit √™tre ajout√©e √† la fonction de d√©cision. Aucune ordonn√©e √† l'origine ne sera utilis√©e dans le calcul et les donn√©es seront suppos√©es √™tre d√©j√† centr√©es, s'il est d√©fini √† false.
- **tol** - flottant ou aucun, facultatif, par d√©faut = 1.e-3. Ce param√®tre repr√©sente le crit√®re d'arr√™t des it√©rations. Sa valeur par d√©faut est False mais s'il est d√©fini sur None, les it√©rations s'arr√™teront lorsque ùíçloss > best_loss - tol pour n_iter_no_changesuccessives √©poques.
- **shuffle** - Bool√©en, facultatif, par d√©faut = True. Ce param√®tre indique si nous voulons que nos donn√©es d'entra√Ænement soient m√©lang√©es apr√®s chaque √©poque ou non.
- **verbose** - entier, par d√©faut = 0. Il repr√©sente le niveau de verbosit√©. Sa valeur par d√©faut est 0.
- **epsilon** - float, default = 0.1. Ce param√®tre sp√©cifie la largeur de la r√©gion insensible. Si loss = 'epsilon-insensitive', toute diff√©rence, entre la pr√©diction actuelle et l'√©tiquette correcte, inf√©rieure au seuil sera ignor√©e.
- **max_iter** - int, optionnel, d√©faut = 1000. Comme son nom l'indique, il repr√©sente le nombre maximum de passages sur les √©poques, c'est-√†-dire les donn√©es d'entra√Ænement.
- **warm_start** - bool, optional, default = false. Avec ce param√®tre r√©gl√© sur True, nous pouvons r√©utiliser la solution de l'appel pr√©c√©dent √† fit comme initialisation. Si nous choisissons la valeur par d√©faut, c'est-√†-dire false, la solution pr√©c√©dente sera effac√©e.
- **random_state** - int, RandomState instance ou None, facultatif, par d√©faut = none. Ce param√®tre repr√©sente la graine du nombre pseudo-al√©atoire g√©n√©r√© qui est utilis√© lors du brassage des donn√©es. Les options suivantes sont disponibles :
    - **int** - Dans ce cas, random_state est la graine utilis√©e par le g√©n√©rateur de nombres al√©atoires.
    - **RandomState instance** - Dans ce cas, random_state est le g√©n√©rateur de nombres al√©atoires.
    - **None** - Dans ce cas, le g√©n√©rateur de nombres al√©atoires est l'instance RandonState utilis√©e par np.random.
- **n_jobs** - int ou none, optionnel, D√©faut = None. Il repr√©sente le nombre de CPUs √† utiliser dans le calcul OVA (One Versus All), pour les probl√®mes multi-classes. La valeur par d√©faut est none, ce qui signifie 1.
- **learning_rate** - cha√Æne de caract√®res, facultatif, par d√©faut = 'optimal'.
Si le taux d'apprentissage est 'constant', eta = eta0 ;
Si le taux d'apprentissage est 'optimal', eta = 1.0/(alpha*(t+t0)), o√π t0 est choisi par Leon Bottou ;
Si le taux d'apprentissage est 'invscalling', eta = eta0/pow(t, power_t).
Si le taux d'apprentissage = 'adaptatif', eta = eta0.
    - **eta0** - double, par d√©faut = 0,0. Il repr√©sente le taux d'apprentissage initial pour les options de taux d'apprentissage mentionn√©es ci-dessus, c'est-√†-dire 'constant', 'invscalling' ou 'adaptive'.
    - **power_t** - idouble, par d√©faut = 0.5. C'est l'exposant pour le taux d'apprentissage 'incscalling'.
    - **early_stopping** - bool, default = False. Ce param√®tre repr√©sente l'utilisation de l'arr√™t pr√©coce pour terminer l'apprentissage lorsque le score de validation ne s'am√©liore pas. Sa valeur par d√©faut est false mais lorsqu'il est r√©gl√© sur true, il met automatiquement de c√¥t√© une fraction stratifi√©e de donn√©es d'apprentissage comme validation et arr√™te l'apprentissage lorsque le score de validation ne s'am√©liore pas.
    - **validation_fraction** - float, default = 0.1. Il n'est utilis√© que lorsque early_stopping est vrai. Il repr√©sente la proportion de donn√©es d'entra√Ænement √† mettre de c√¥t√© comme ensemble de validation pour l'arr√™t pr√©coce des donn√©es d'entra√Ænement....
- **n_iter_no_change** - int, default=5. Il repr√©sente le nombre d'it√©rations sans am√©lioration que l'algorithme doit ex√©cuter avant l'arr√™t pr√©coce.
- **class_weight** - dict, {class_label : weight} ou "balanced", ou None, optional. Ce param√®tre repr√©sente les poids associ√©s aux classes. S'il n'est pas fourni, les classes sont suppos√©es avoir le poids 1.
- **warm_start** - bool, optionnel, par d√©faut = false. Avec ce param√®tre r√©gl√© sur True, nous pouvons r√©utiliser la solution de l'appel pr√©c√©dent √† fit comme initialisation. Si nous choisissons la valeur par d√©faut, c'est-√†-dire false, cela effacera la solution pr√©c√©dente.
- **average** - iBool√©en ou int, facultatif, par d√©faut = false. Il repr√©sente le nombre de CPU √† utiliser dans le calcul OVA (One Versus All), pour les probl√®mes multi-classes. La valeur par d√©faut est none, ce qui signifie 1.

### Attributs

La liste suivante pr√©sente les attributs utilis√©s par le module SGDClassifier :

- **coef_** - tableau, forme (1, n_features) si n_classes==2, sinon (n_classes, n_features). Cet attribut fournit le poids attribu√© aux caract√©ristiques.
- **intercept_** - tableau, forme (1,) si n_classes==2, sinon (n_classes,). Il repr√©sente le terme ind√©pendant dans la fonction de d√©cision.
- **n_iter_** - int. Il donne le nombre d'it√©rations pour atteindre le crit√®re d'arr√™t.

### Exemples d'impl√©mentation

Comme d'autres classificateurs, la descente par gradient stochastique (SGD) doit √™tre √©quip√©e des deux tableaux suivants : 

- Un tableau X contenant les √©chantillons d'entra√Ænement. Il est de taille [n_samples, n_features].
- Un tableau Y contenant les valeurs cibles, c'est-√†-dire les √©tiquettes de classe pour les √©chantillons d'apprentissage. Il est de taille [n_√©chantillons].

#### Exemple

Le script Python suivant utilise le mod√®le lin√©aire SGDClassifier :

```python
import numpy as np
from sklearn import linear_model
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
Y = np.array([1, 1, 2, 2])
SGDClf = linear_model.SGDClassifier(max_iter = 1000, tol=1e-3,penalty = "elasticnet")
SGDClf.fit(X, Y)
```

#### Sortie

```python
SGDClassifier(
    alpha = 0.0001, average = False, class_weight = None,
    early_stopping = False, epsilon = 0.1, eta0 = 0.0, fit_intercept = True,
    l1_ratio = 0.15, learning_rate = 'optimal', loss = 'hinge', max_iter = 1000,
    n_iter = None, n_iter_no_change = 5, n_jobs = None, penalty = 'elasticnet',
    power_t = 0.5, random_state = None, shuffle = True, tol = 0.001,
    validation_fraction = 0.1, verbose = 0, warm_start = False
)
```

#### Exemple

Maintenant, une fois ajust√©, le mod√®le peut pr√©dire de nouvelles valeurs comme suit :

```python
SGDClf.predict([[2.,2.]])
```

#### Sortie

```python
array([2])
```

#### Exemple

Pour l'exemple ci-dessus, nous pouvons obtenir le vecteur poids √† l'aide du script python suivant :

```python
SGDClf.coef_
```

#### Sortie

```python
array([[19.54811198, 9.77200712]])
```

#### Exemple

De m√™me, nous pouvons obtenir la valeur d'intercept √† l'aide du script python suivant :

```python
SGDClf.intercept_
```

#### Sortie

```python
array([10.])
```

#### Exemple

Nous pouvons obtenir la distance sign√©e par rapport √† l'hyperplan en utilisant SGDClassifier.decision_function, comme dans le script python suivant :

```python
SGDClf.decision_function([[2., 2.]])
```

#### Sortie

```python
array([68.6402382])
```

## R√©gresseur SGD

Le r√©gresseur SGD (Stochastic Gradient Descent) impl√©mente essentiellement une routine d'apprentissage SGD simple prenant en charge diverses fonctions de perte et p√©nalit√©s pour ajuster les mod√®les de r√©gression lin√©aire. Scikit-learn fournit le module **SGDRegressor** pour mettre en ≈ìuvre la r√©gression SGD.

### Param√®tres

Les param√®tres utilis√©s par **SGDRegressor** sont presque les m√™mes que ceux utilis√©s dans le module **SGDClassifier**. La diff√©rence r√©side dans le param√®tre 'loss'. Pour le param√®tre de perte du module **SGDRegressor** les valeurs positives sont les suivantes :

- **squared_loss** - Il s'agit de l'ajustement ordinaire des moindres carr√©s.
- **huber : SGDRegressor** - corrige les valeurs aberrantes en passant de la perte quadratique √† la perte lin√©aire au-del√† d'une distance d'epsilon. Le travail de 'huber' est de modifier 'squared_loss' pour que l'algorithme se concentre moins sur la correction des valeurs aberrantes.
- **epsilon_insensitive** - En fait, il ignore les erreurs inf√©rieures √† epsilon.
- **squared_epsilon_insensitive** - C'est la m√™me chose que epsilon_insensitive. La seule diff√©rence est qu'elle devient une perte au carr√© au-del√† d'une tol√©rance d'epsilon.

Une autre diff√©rence est que le param√®tre nomm√© 'power_t' a la valeur par d√©faut de 0.25 plut√¥t que 0.5 comme dans SGDClassifier. De plus, il n'a pas les param√®tres 'class_weight' et 'n_jobs'.

### Attributs

Les attributs de SGDRegressor sont √©galement les m√™mes que ceux du module SGDClassifier. En revanche, il poss√®de trois attributs suppl√©mentaires, √† savoir :

- **average_coef_** - tableau, forme(n_features,). Comme son nom l'indique, il fournit les poids moyens attribu√©s aux caract√©ristiques.
- **average_intercept_** - tableau, forme(1,). Comme son nom l'indique, il fournit le terme d'interception moyen.
- **t_** - int. Il fournit le nombre de mises √† jour de poids effectu√©es pendant la phase d'apprentissage.

Les attributs average_coef_ et average_intercept_ fonctionneront apr√®s avoir activ√© le param√®tre 'average' sur True.

### Exemple de mise en ≈ìuvre

#### Exemple

Le script Python suivant utilise le mod√®le lin√©aire SGDRegressor :

```python
import numpy as np
from sklearn import linear_model
n_samples, n_features = 10, 5
rng = np.random.RandomState(0)
y = rng.randn(n_samples)
X = rng.randn(n_samples, n_features)
SGDReg =linear_model.SGDRegressor(
    max_iter = 1000,penalty = "elasticnet",loss = 'huber',tol = 1e-3, average =  True
)
SGDReg.fit(X, y)
```

#### Sortie

```python
SGDRegressor(
    alpha = 0.0001, average = True, early_stopping = False, epsilon = 0.1,
    eta0 = 0.01, fit_intercept = True, l1_ratio = 0.15,
    learning_rate = 'invscaling', loss = 'huber', max_iter = 1000,
    n_iter = None, n_iter_no_change = 5, penalty = 'elasticnet', power_t = 0.25,
    random_state = None, shuffle = True, tol = 0.001, validation_fraction = 0.1,
    verbose = 0, warm_start = False
)
```

#### Exemple

Maintenant, une fois ajust√©, nous pouvons obtenir le vecteur de poids √† l'aide du script python suivant :

````python
SGDReg.coef_
````

#### Sortie

````python
array([-0.00423314, 0.00362922, -0.00380136, 0.00585455, 0.00396787])
````

#### Exemple

De m√™me, nous pouvons obtenir la valeur d'intercept √† l'aide du script python suivant -

````python
SGReg.intercept_
````

#### Sortie

````python
SGReg.intercept_
````

#### Exemple

Nous pouvons obtenir le nombre de mises √† jour de poids pendant la phase de formation √† l'aide du script python suivant :

````python
SGDReg.t_
````

#### Sortie

````python
61.0
````

## Avantages et inconv√©nients du SGD

### Avantages

- La descente de gradient stochastique (SGD) est tr√®s efficace.
- Il est tr√®s facile √† mettre en ≈ìuvre car il existe de nombreuses possibilit√©s d'ajustement du code.

### Inconv√©nients

- La descente stochastique par gradient (SGD) n√©cessite plusieurs hyperparam√®tres comme les param√®tres de r√©gularisation.
- Elle est sensible √† l'√©chelle des caract√©ristiques.