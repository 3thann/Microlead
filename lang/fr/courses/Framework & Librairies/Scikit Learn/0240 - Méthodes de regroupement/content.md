## Introduction

Ici, nous allons √©tudier les m√©thodes de clustering dans Sklearn qui aideront √† identifier toute similarit√© dans les √©chantillons de donn√©es.

Les m√©thodes de regroupement, l'une des m√©thodes de ML non supervis√©es les plus utiles, sont utilis√©es pour trouver des mod√®les de similarit√© et de relation entre les √©chantillons de donn√©es. Ensuite, elles regroupent ces √©chantillons en groupes ayant une similarit√© bas√©e sur des caract√©ristiques. Le clustering d√©termine le regroupement intrins√®que entre les donn√©es non √©tiquet√©es actuelles, d'o√π son importance.

La biblioth√®que Scikit-learn poss√®de sklearn.cluster pour effectuer le clustering de donn√©es non √©tiquet√©es. Sous ce module, scikit-leran dispose des m√©thodes de clustering suivantes -

## KMeans

Cet algorithme calcule les centro√Ødes et it√®re jusqu'√† ce qu'il trouve le centro√Øde optimal. Il exige que le nombre de groupes soit sp√©cifi√©, c'est pourquoi il suppose qu'il est d√©j√† connu. La logique principale de cet algorithme est de regrouper les √©chantillons de s√©paration des donn√©es dans un nombre n de groupes de variances √©gales en minimisant le crit√®re connu sous le nom d'inertie. Le nombre de groupes identifi√©s par l'algorithme est repr√©sent√© par 'K'.

Scikit-learn dispose du module sklearn.cluster.KMeans pour effectuer le clustering K-Means. Lors du calcul des centres de clusters et de la valeur d'inertie, le param√®tre nomm√© sample_weight permet au module sklearn.cluster.KMeans d'attribuer plus de poids √† certains √©chantillons.

## Affinity Propagation

Cet algorithme est bas√© sur le concept de "passage de messages" entre diff√©rentes paires d'√©chantillons jusqu'√† convergence. Il ne n√©cessite pas de sp√©cifier le nombre de clusters avant d'ex√©cuter l'algorithme. L'algorithme a une complexit√© temporelle de l'ordre de ùëÇ(ùëÅ2ùëá), ce qui constitue son plus grand inconv√©nient.

Scikit-learn dispose du module sklearn.cluster.AffinityPropagation pour effectuer le clustering par propagation d'affinit√©.

## Mean Shift

Cet algorithme d√©couvre principalement des blobs dans une densit√© lisse d'√©chantillons. Il affecte les points de donn√©es aux clusters de mani√®re it√©rative en d√©pla√ßant les points vers la densit√© la plus √©lev√©e de points de donn√©es. Au lieu de s'appuyer sur un param√®tre appel√© bande passante qui dicte la taille de la r√©gion √† parcourir, il d√©finit automatiquement le nombre de clusters.

Scikit-learn dispose du module sklearn.cluster.MeanShift pour r√©aliser le clustering Mean Shift.

## Spectral Clustering

Avant le clustering, cet algorithme utilise essentiellement les valeurs propres, c'est-√†-dire le spectre de la matrice de similarit√© des donn√©es, pour effectuer une r√©duction de la dimensionnalit√© en moins de dimensions. L'utilisation de cet algorithme n'est pas conseill√©e lorsqu'il y a un grand nombre de clusters.

Scikit-learn dispose du module sklearn.cluster.SpectralClustering pour effectuer un clustering spectral.

## Hierarchical Clustering

Cet algorithme construit des clusters imbriqu√©s en fusionnant ou en divisant les clusters successivement. Cette hi√©rarchie de clusters est repr√©sent√©e sous forme de dendrogramme, c'est-√†-dire d'arbre. Elle se divise en deux cat√©gories -

Agglomerative hierarchical algorithms ‚àí Dans ce type d'algorithme hi√©rarchique, chaque point de donn√©es est trait√© comme un seul cluster. Il agglom√®re ensuite successivement les paires de clusters. Cette m√©thode utilise l'approche ascendante.

Divisive hierarchical algorithms ‚àí Dans cet algorithme hi√©rarchique, tous les points de donn√©es sont trait√©s comme un seul grand cluster. Le processus de mise en grappes consiste √† diviser, en utilisant une approche descendante, le grand groupe en plusieurs petits groupes.

Scikit-learn dispose du module sklearn.cluster.AgglomerativeClustering pour effectuer un clustering hi√©rarchique agglom√©ratif.

## DBSCAN

Il s'agit de l'acronyme "Density-based spatial clustering of applications with noise". Cet algorithme est bas√© sur la notion intuitive de "clusters" et de "bruit", selon laquelle les clusters sont des r√©gions denses de plus faible densit√© dans l'espace des donn√©es, s√©par√©es par des r√©gions de plus faible densit√© de points de donn√©es.

Scikit-learn dispose du module sklearn.cluster.DBSCAN pour effectuer le clustering DBSCAN. Il existe deux param√®tres importants, √† savoir min_samples et eps, utilis√©s par cet algorithme pour d√©finir la densit√©.

Une valeur plus √©lev√©e du param√®tre min_samples ou une valeur plus faible du param√®tre eps donnera une indication sur la densit√© plus √©lev√©e des points de donn√©es qui est n√©cessaire pour former un cluster.

## OPTICS

Il s'agit de l'acronyme anglais "Ordering points to identify the clustering structure". Cet algorithme trouve √©galement des clusters bas√©s sur la densit√© dans les donn√©es spatiales. Sa logique de fonctionnement de base est similaire √† celle de DBSCAN.

Il r√©pond √† l'une des principales faiblesses de l'algorithme DBSCAN - le probl√®me de la d√©tection de groupes significatifs dans des donn√©es de densit√© variable - en ordonnant les points de la base de donn√©es de mani√®re √† ce que les points les plus proches dans l'espace deviennent des voisins dans l'ordre.

Scikit-learn dispose du module sklearn.cluster.OPTICS pour effectuer le clustering OPTICS.

## BIRCH

Il s'agit de l'acronyme anglais Balanced iterative reducing and clustering using hierarchies. Il est utilis√© pour effectuer un clustering hi√©rarchique sur de grands ensembles de donn√©es. Il construit un arbre nomm√© CFT (Characteristics Feature Tree) pour les donn√©es donn√©es.

L'avantage de la CFT est que les n≈ìuds de donn√©es appel√©s n≈ìuds CF (Characteristics Feature) contiennent les informations n√©cessaires √† la mise en grappes, ce qui √©vite de devoir conserver en m√©moire l'ensemble des donn√©es d'entr√©e.

Scikit-learn dispose du module sklearn.cluster.Birch pour effectuer le clustering BIRCH.

## Comparaison des algorithmes de clustering

Le tableau suivant donne une comparaison (bas√©e sur les param√®tres, l'√©volutivit√© et la m√©trique) des algorithmes de clustering dans scikit-learn.

| **Nom de l'algorithme** | **Param√®tres** | **Scalabilit√©** | **M√©trique utilis√©e** |
| --- | --- | --- | --- |
| K-Means | Nombre de clusters | Tr√®s grand n_√©chantillons | La distance entre les points. |
| Affinity Propagation | Amortissement | Ce n'est pas extensible avec n_√©chantillons. | Distance du graphique |
| Mean-Shift | Bande passante | Ce n'est pas extensible avec n_√©chantillons. | La distance entre les points. |
| Spectral Clustering | Nombre de clusters | Niveau moyen d'extensibilit√© avec n_√©chantillons. Petit niveau d'extensibilit√© avec n_clusters. | Distance du graphique |
| Hierarchical Clustering | Seuil de distance ou nombre de clusters | Grand n_√©chantillons Grand n_clusters | La distance entre les points. |
| DBSCAN | Taille du quartier | Tr√®s grands n_√©chantillons et n_clusters moyens. | Distance du point le plus proche |
| OPTICS | Adh√©sion minimale au cluster | Tr√®s grands n_√©chantillons et grands n_clusters. | La distance entre les points. |
| BIRCH | Seuil, facteur de branchement | Grand n_√©chantillons Grand n_clusters | La distance euclidienne entre les points. |

## Clustering K-Means sur l'ensemble de donn√©es Scikit-learn Digit

Dans cet exemple, nous allons appliquer le clustering K-means au jeu de donn√©es des chiffres. Cet algorithme identifiera les chiffres similaires sans utiliser les informations de l'√©tiquette originale. L'impl√©mentation est faite sur Jupyter notebook.

```python
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np
from sklearn.cluster import KMeans
from sklearn.datasets import load_digits
digits = load_digits()
digits.data.shape
```

### Sortie

```python
1797, 64)
```

Cette sortie montre que l'ensemble de donn√©es num√©riques a 1797 √©chantillons avec 64 caract√©ristiques.

### Exemple

Maintenant, effectuez le clustering K-Means comme suit -

```python
kmeans = KMeans(n_clusters = 10, random_state = 0)
clusters = kmeans.fit_predict(digits.data)
kmeans.cluster_centers_.shape
```

### Sortie

```python
(10, 64)
```

Cette sortie montre que le clustering K-means a cr√©√© 10 clusters avec 64 caract√©ristiques.

### Exemple

```python
fig, ax = plt.subplots(2, 5, figsize = (8, 3))
centers = kmeans.cluster_centers_.reshape(10, 8, 8)
for axi, center in zip(ax.flat, centers):
axi.set(xticks = [], yticks = [])
axi.imshow(center, interpolation = 'nearest', cmap = plt.cm.binary)
```

### Sortie

La sortie ci-dessous contient des images montrant les centres des clusters appris par K-Means Clustering.

![Image montrant les centres des clusters appris par K-Means Clustering](https://raw.githubusercontent.com/Microleadoff/content/master/lang/fr/courses/Framework%20%26%20Librairies/Scikit%20Learn/0240%20-%20M%C3%A9thodes%20de%20regroupement/images/image1.jpg)

Ensuite, le script Python ci-dessous fera correspondre les √©tiquettes de clusters apprises (par K-Means) avec les v√©ritables √©tiquettes trouv√©es dans ces clusters.

```python
from scipy.stats import mode
labels = np.zeros_like(clusters)
for i in range(10):
mask = (clusters == i)
labels[mask] = mode(digits.target[mask])[0]
```

Nous pouvons √©galement v√©rifier la pr√©cision avec l'aide de la commande mentionn√©e ci-dessous.

```python
from sklearn.metrics import accuracy_score
accuracy_score(digits.target, labels)
```

Sortie

```python
0.7935447968836951
```

## Exemple de mise en ≈ìuvre compl√®te

```python
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np

from sklearn.cluster import KMeans
from sklearn.datasets import load_digits
digits = load_digits()
digits.data.shape
kmeans = KMeans(n_clusters = 10, random_state = 0)
clusters = kmeans.fit_predict(digits.data)
kmeans.cluster_centers_.shape
fig, ax = plt.subplots(2, 5, figsize = (8, 3))
centers = kmeans.cluster_centers_.reshape(10, 8, 8)
for axi, center in zip(ax.flat, centers):
   axi.set(xticks=[], yticks = [])
   axi.imshow(center, interpolation = 'nearest', cmap = plt.cm.binary)
from scipy.stats import mode
labels = np.zeros_like(clusters)
for i in range(10):
   mask = (clusters == i)
   labels[mask] = mode(digits.target[mask])[0]
from sklearn.metrics import accuracy_score
accuracy_score(digits.target, labels)
```