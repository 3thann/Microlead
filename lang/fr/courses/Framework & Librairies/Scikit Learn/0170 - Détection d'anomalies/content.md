## Introduction

La d√©tection des anomalies est une technique utilis√©e pour identifier les points de donn√©es dans un ensemble de donn√©es qui ne correspondent pas au reste des donn√©es. Elle a de nombreuses applications dans le monde des affaires, comme la d√©tection des fraudes, la d√©tection des intrusions, le contr√¥le de la sant√© des syst√®mes, la surveillance et la maintenance pr√©dictive. Les anomalies, √©galement appel√©es valeurs aberrantes, peuvent √™tre divis√©es en trois cat√©gories, √† savoir

- **Point anomalies** ‚àí Cela se produit lorsqu'une instance de donn√©es individuelle est consid√©r√©e comme anormale par rapport au reste des donn√©es.
- **Contextual anomalies** ‚àí Ce type d'anomalie est sp√©cifique au contexte. Elle se produit si une instance de donn√©es est anormale dans un contexte sp√©cifique.
- **Collective anomalies** ‚àí Cela se produit lorsqu'une collection d'instances de donn√©es li√©es est anormale par rapport √† l'ensemble des donn√©es plut√¥t que par rapport aux valeurs individuelles.

## Methods

Deux m√©thodes, √† savoir la d√©tection des valeurs Outlier et la d√©tection des Novelty , peuvent √™tre utilis√©es pour la d√©tection des anomalies. Il est n√©cessaire de faire la distinction entre ces deux m√©thodes.

### Outlier detection

Les donn√©es de formation contiennent des valeurs aberrantes qui sont √©loign√©es du reste des donn√©es. Ces valeurs aberrantes sont d√©finies comme des observations. C'est la raison pour laquelle les estimateurs de d√©tection des valeurs aberrantes essaient toujours de s'adapter √† la r√©gion pr√©sentant les donn√©es d'apprentissage les plus concentr√©es tout en ignorant les observations d√©viantes. Cette m√©thode est √©galement connue sous le nom de d√©tection d'anomalie non supervis√©e.

### Novelty detection

Il s'agit de d√©tecter un mod√®le non observ√© dans les nouvelles observations qui ne sont pas incluses dans les donn√©es de formation. Ici, les donn√©es de formation ne sont pas pollu√©es par les valeurs aberrantes. Elle est √©galement connue sous le nom de d√©tection d'anomalie semi-supervis√©e.

Il existe un ensemble d'outils ML, fournis par scikit-learn, qui peuvent √™tre utilis√©s √† la fois pour la d√©tection des aberrations et pour la d√©tection des nouveaut√©s. Ces outils mettent d'abord en ≈ìuvre l'apprentissage d'objets √† partir des donn√©es de mani√®re non supervis√©e en utilisant la m√©thode fit (), comme suit .

```python
estimator.fit(X_train)
```

Maintenant, les nouvelles observations seront tri√©es en tant que valeurs inliers (√©tiquet√©es 1) ou outliers (√©tiquet√©es -1) en utilisant la m√©thode predict() comme suit -

```python
estimator.fit(X_test)
```

L'estimateur calcule d'abord la fonction de notation brute, puis la m√©thode de pr√©diction utilise un seuil sur cette fonction de notation brute. Nous pouvons acc√©der √† cette fonction de notation brute √† l'aide de la m√©thode score_sample et nous pouvons contr√¥ler le seuil par le param√®tre de contamination.

Nous pouvons √©galement d√©finir la m√©thode decision_function qui d√©finit les valeurs aberrantes comme des valeurs n√©gatives et les valeurs aberrantes comme des valeurs non n√©gatives.

```python
estimator.decision_function(X_test)
```

## Algorithmes de Sklearn pour la d√©tection de valeurs aberrantes

Commen√ßons par comprendre ce qu'est une enveloppe elliptique.

### Ajuster une enveloppe elliptique

Cet algorithme suppose que les donn√©es r√©guli√®res proviennent d'une distribution connue, telle que la distribution gaussienne. Pour la d√©tection des valeurs aberrantes, Scikit-learn fournit un objet nomm√© covariance.EllipticEnvelop.

Cet objet ajuste une estimation robuste de la covariance aux donn√©es, et donc, ajuste une ellipse aux points centraux des donn√©es. Il ignore les points situ√©s en dehors du mode central.

#### Param√®tres

Le tableau suivant pr√©sente les param√®tres utilis√©s par sklearn. covariance.m√©thode EllipticEnvelop -

**Param√®tre et description**

- **store_precision** ‚àí Boolean, optional, default = True - Nous pouvons le sp√©cifier si la pr√©cision estim√©e est stock√©e.
- **assume_centered** ‚àí Boolean, optional, default = False - Si nous le mettons √† Faux, il calculera l'emplacement robuste et la covariance directement avec l'aide de l'algorithme FastMCD. D'autre part, si on le met √† True, il calculera le support de l'emplacement robuste et de la covariance.
- **support_fraction** ‚àí float in (0., 1.), optional, default = None - Ce param√®tre indique √† la m√©thode quelle proportion de points doit √™tre incluse dans le support des estimations MCD brutes.
- **contamination** ‚àí float in (0., 1.), optional, default = 0.1 - Il fournit la proportion des valeurs aberrantes dans l'ensemble des donn√©es.
- **random_state** ‚àí int, RandomState instance or None, optional, default = none - Ce param√®tre repr√©sente la graine du nombre pseudo-al√©atoire g√©n√©r√© qui est utilis√© lors du brassage des donn√©es. Les options suivantes sont disponibles
    - **int** ‚àí Dans ce cas, random_state est la graine utilis√©e par le g√©n√©rateur de nombres al√©atoires.
    - **RandomState instance** ‚àí Dans ce cas, random_state est le g√©n√©rateur de nombres al√©atoires.
    - **None** ‚àí Dans ce cas, le g√©n√©rateur de nombres al√©atoires est l'instance RandonState utilis√©e par np.random.

#### Attributes

Le tableau suivant pr√©sente les attributs utilis√©s par sklearn. covariance.m√©thode EllipticEnvelop -

**Attributs et description**

- **support_** ‚àí array-like, shape(n_samples,) - Il repr√©sente le masque des observations utilis√©es pour calculer des estimations robustes de l'emplacement et de la forme.
- **location_** ‚àí array-like, shape (n_features) - Il renvoie l'emplacement robuste estim√©.
- **covariance_** ‚àí array-like, shape (n_features, n_features) - Il renvoie la matrice de covariance robuste estim√©e.
- **precision_** ‚àí array-like, shape (n_features, n_features) - Il renvoie la matrice pseudo-inverse estim√©e.
- **offset_** ‚àí float - Elle est utilis√©e pour d√©finir la fonction de d√©cision √† partir des scores bruts. fonction_de_d√©cision = score_samples -offset_

#### Exemple de mise en ≈ìuvre

```python
import numpy as np^M
from sklearn.covariance import EllipticEnvelope^M
true_cov = np.array([[.5, .6],[.6, .4]])
X = np.random.RandomState(0).multivariate_normal(mean = [0, 0], cov=true_cov,size=500)
cov = EllipticEnvelope(random_state = 0).fit(X)^M
# Now we can use predict method. It will return 1 for an inlier and -1 for an outlier.
cov.predict([[0, 0],[2, 2]])
```

#### Sortie

```python
array([ 1, -1])
```

## Isolation Forest

Dans le cas d'un ensemble de donn√©es √† haute dimension, une fa√ßon efficace de d√©tecter les valeurs aberrantes est d'utiliser les for√™ts al√©atoires. Le scikit-learn fournit la m√©thode ensemble.IsolationForest qui isole les observations en s√©lectionnant al√©atoirement une caract√©ristique. Ensuite, elle s√©lectionne de mani√®re al√©atoire une valeur entre les valeurs maximale et minimale des caract√©ristiques s√©lectionn√©es.

Ici, le nombre de fractionnements n√©cessaires pour isoler un √©chantillon est √©quivalent √† la longueur du chemin entre le n≈ìud racine et le n≈ìud terminateur.

### Param√®tres

Le tableau suivant pr√©sente les param√®tres utilis√©s par la m√©thode ensemble.IsolationForest de sklearn.

**Param√®tre et description**

- **n_estimators** ‚àí int, optional, default = 100 - Il repr√©sente le nombre d'estimateurs de base dans l'ensemble.
- **max_samples** ‚àí int or float, optional, default = ‚Äúauto‚Äù - Il repr√©sente le nombre d'√©chantillons √† tirer de X pour entra√Æner chaque estimateur de base. Si nous choisissons int comme valeur, il tirera max_samples √©chantillons. Si nous choisissons float comme valeur, il tirera des √©chantillons max_samples ‚àó ùëã.shape[0]. Et, si nous choisissons auto comme valeur, il dessinera max_√©chantillons = min(256,n_√©chantillons).
- **support_fraction** ‚àí float in (0., 1.), optional, default = None - Ce param√®tre indique √† la m√©thode quelle proportion de points doit √™tre incluse dans le support des estimations MCD brutes.
- **contamination** ‚àí auto or float, optional, default = auto - Il fournit la proportion de valeurs aberrantes dans l'ensemble de donn√©es. Si nous le d√©finissons par d√©faut, c'est-√†-dire auto, il d√©terminera le seuil comme dans l'article original. S'il est d√©fini sur float, la gamme de contamination sera dans la gamme de [0,0.5].
- **random_state** ‚àí int, RandomState instance or None, optional, default = none - Ce param√®tre repr√©sente la graine du nombre pseudo-al√©atoire g√©n√©r√© qui est utilis√© lors du brassage des donn√©es. Les options suivantes sont disponibles
    - **int** ‚àí Dans ce cas, random_state est la graine utilis√©e par le g√©n√©rateur de nombres al√©atoires.
    - **RandomState instance** ‚àí Dans ce cas, random_state est le g√©n√©rateur de nombres al√©atoires.
    - **None** ‚àí Dans ce cas, le g√©n√©rateur de nombres al√©atoires est l'instance RandonState utilis√©e par np.random.
- **max_features** ‚àí int or float, optional (default = 1.0) - Il repr√©sente le nombre de caract√©ristiques √† tirer de X pour entra√Æner chaque estimateur de base. Si nous choisissons int comme valeur, il tirera les caract√©ristiques max_features. Si nous choisissons float comme valeur, il tirera max_features * X.shape[ùüè] √©chantillons.
- **bootstrap** ‚àí Boolean, optional (default = False) - Son option par d√©faut est False, ce qui signifie que l'√©chantillonnage est effectu√© sans remplacement. D'autre part, si elle est d√©finie sur True, cela signifie que les arbres individuels sont ajust√©s sur un sous-ensemble al√©atoire des donn√©es d'apprentissage √©chantillonn√© avec remplacement.
- **n_jobs** ‚àí int or None, optional (default = None) - Il repr√©sente le nombre de t√¢ches √† ex√©cuter en parall√®le pour les m√©thodes fit() et predict().
- **verbose** ‚àí int, optional (default = 0) - Ce param√®tre contr√¥le la verbosit√© du processus de construction de l'arbre.
- **warm_start** ‚àí Bool, optional (default=False) - Si warm_start = true, nous pouvons r√©utiliser la solution des appels pr√©c√©dents pour ajuster et nous pouvons ajouter plus d'estimateurs √† l'ensemble. Mais s'il est d√©fini √† false, nous devons ajuster une toute nouvelle for√™t.

### Attributes

Le tableau suivant contient les attributs utilis√©s par la m√©thode ensemble.IsolationForest de sklearn -

**Attributs et description**

- **estimators_** ‚àí list of DecisionTreeClassifier - Fournir la collection de tous les sous-estimateurs adapt√©s.
- **max_samples_** ‚àí integer - Il fournit le nombre r√©el d'√©chantillons utilis√©s.
- **offset_** ‚àí float - Elle est utilis√©e pour d√©finir la fonction de d√©cision √† partir des scores bruts. fonction_de_d√©cision = score_samples -offset_

### Exemple de mise en ≈ìuvre

Le script Python ci-dessous utilisera la m√©thode sklearn. ensemble.IsolationForest pour ajuster 10 arbres sur des donn√©es donn√©es donn√©es.

```python
from sklearn.ensemble import IsolationForest
import numpy as np
X = np.array([[-1, -2], [-3, -3], [-3, -4], [0, 0], [-50, 60]])
OUTDClf = IsolationForest(n_estimators = 10)
OUTDclf.fit(X)
```

### Sortie

```python
IsolationForest(
    behaviour = 'old', bootstrap = False, contamination='legacy',
    max_features = 1.0, max_samples = 'auto', n_estimators = 10, n_jobs=None,
    random_state = None, verbose = 0
)
```

## Local Outlier Factor

L'algorithme LOF (Local Outlier Factor) est un autre algorithme efficace pour effectuer la d√©tection des aberrations sur des donn√©es de haute dimension. Le scikit-learn fournit la m√©thode neighbors.LocalOutlierFactor qui calcule un score, appel√© facteur d'aberration locale, refl√©tant le degr√© d'anomalie des observations. La logique principale de cet algorithme est de d√©tecter les √©chantillons qui ont une densit√© sensiblement inf√©rieure √† celle de leurs voisins. C'est pourquoi il mesure l'√©cart de densit√© locale de points de donn√©es donn√©s par rapport √† leurs voisins.

### Param√®tres

Le tableau suivant contient les param√®tres utilis√©s par la m√©thode sklearn. neighbors.LocalOutlierFactor

**Param√®tre et description**

- **n_neighbors** ‚àí int, optional, default = 20 - Il repr√©sente le nombre de voisins utilis√©s par d√©faut pour la requ√™te kneighbors. Tous les √©chantillons seront utilis√©s si .
- **algorithm** ‚àí optional - 
    - Quel algorithme utiliser pour calculer les plus proches voisins.
    - Si vous choisissez ball_tree, il utilisera l'algorithme BallTree.
    - Si vous choisissez kd_tree, il utilisera l'algorithme KDTree.
    - Si vous choisissez brute, il utilisera l'algorithme de recherche par force brute.
    - Si vous choisissez auto, il d√©cidera de l'algorithme le plus appropri√© sur la base de la valeur que nous avons pass√©e √† la m√©thode fit().
- **leaf_size** ‚àí int, optional, default = 30 - The value of this parameter can affect the speed of the construction and query. It also affects the memory required to store the tree. This parameter is passed to BallTree or KdTree algorithms.
- **contamination** ‚àí auto or float, optional, default = auto - Il fournit la proportion de valeurs aberrantes dans l'ensemble de donn√©es. Si nous le d√©finissons par d√©faut, c'est-√†-dire auto, il d√©terminera le seuil comme dans l'article original. S'il est d√©fini sur float, la gamme de contamination sera dans la gamme de [0,0.5].
- **metric** ‚àí string or callable, default - Il repr√©sente la m√©trique utilis√©e pour le calcul de la distance.
- **P** ‚àí int, optional (default = 2) - Il s'agit du param√®tre de la m√©trique de Minkowski. P=1 √©quivaut √† l'utilisation de la distance manhattanienne, c'est-√†-dire L1, tandis que P=2 √©quivaut √† l'utilisation de la distance euclidienne, c'est-√†-dire L2.
- **novelty** ‚àí Boolean, (default = False) - Par d√©faut, l'algorithme LOF est utilis√© pour la d√©tection des valeurs aberrantes mais il peut √™tre utilis√© pour la d√©tection de la nouveaut√© si nous d√©finissons nouveaut√© = vrai.
- **n_jobs** ‚àí int or None, optional (default = None) - Il repr√©sente le nombre de t√¢ches √† ex√©cuter en parall√®le pour les m√©thodes fit() et predict().

### Attributs

Le tableau suivant pr√©sente les attributs utilis√©s par la m√©thode sklearn.neighbors.LocalOutlierFactor.

**Attributs et description**

- **negative_outlier_factor_** ‚àí numpy array, shape(n_samples,) - Fournir le LOF oppos√© des √©chantillons d'entra√Ænement.
- **n_neighbors_** ‚àí integer - Il fournit le nombre r√©el de voisins utilis√©s pour les requ√™tes de voisins.
- **offset_** ‚àí float - Il est utilis√© pour d√©finir les √©tiquettes binaires √† partir des scores bruts.

### Exemple de mise en ≈ìuvre

Le script Python ci-dessous utilisera la m√©thode

```sklearn.neighbors.LocalOutlierFactor``` pour construire la classe NeighborsClassifier √† partir d'un tableau correspondant √† notre ensemble de donn√©es.

```python
from sklearn.neighbors import NearestNeighbors
samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]
LOFneigh = NearestNeighbors(n_neighbors = 1, algorithm = "ball_tree",p=1)
LOFneigh.fit(samples)
```

### Sortie

```python
NearestNeighbors(
    algorithm = 'ball_tree', leaf_size = 30, metric='minkowski',
    metric_params = None, n_jobs = None, n_neighbors = 1, p = 1, radius = 1.0
)
```

### Exemple

Maintenant, nous pouvons demander √† partir de ce classificateur construit si le point de fermeture se situe dans [0.5, 1., 1.5] en utilisant le script python suivant -

```python
print(neigh.kneighbors([[.5, 1., 1.5]])
```

### Sortie

```python
(array([[1.7]]), array([[1]], dtype = int64))
```

## One-Class SVM

Le SVM √† une classe, introduit par Sch√∂lkopf et al, permet la d√©tection non supervis√©e des aberrations. Il est √©galement tr√®s efficace dans les donn√©es √† haute dimension et estime le support d'une distribution √† haute dimension. Elle est impl√©ment√©e dans le module Support Vector Machines dans l'objet Sklearn.svm.OneClassSVM. Pour d√©finir une fronti√®re, il faut un noyau (le plus souvent utilis√© est le RBF) et un param√®tre scalaire.

Pour une meilleure compr√©hension, ajustons nos donn√©es avec l'objet svm.OneClassSVM -.

### Exemple

```python
from sklearn.svm import OneClassSVM
X = [[0], [0.89], [0.90], [0.91], [1]]
OSVMclf = OneClassSVM(gamma = 'scale').fit(X)
```

Maintenant, nous pouvons obtenir le score_samples pour les donn√©es d'entr√©e comme suit -

```python
OSVMclf.score_samples(X)
```

### Sortie

```python
array([1.12218594, 1.58645126, 1.58673086, 1.58645127, 1.55713767])
```