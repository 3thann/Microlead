La r√©gression bay√©sienne permet un m√©canisme naturel pour survivre √† des donn√©es insuffisantes ou mal distribu√©es en formulant la r√©gression lin√©aire en utilisant des distributions de probabilit√© plut√¥t que des estimations ponctuelles. La sortie ou la r√©ponse "y" est suppos√©e √™tre tir√©e d'une distribution de probabilit√© plut√¥t qu'estim√©e comme une valeur unique.

Math√©matiquement, pour obtenir un mod√®le enti√®rement probabiliste, la r√©ponse y est suppos√©e √™tre distribu√©e de mani√®re gaussienne autour de Xwùëã comme suit :

![mod√®le math√©matique probabiliste](https://raw.githubusercontent.com/Microleadoff/content/master/lang/fr/courses/Framework%20%26%20Librairies/Scikit%20Learn/0090%20-%20Mod%C3%A8le%20lin%C3%A9aire%20%3A%20R%C3%A9gression%20en%20cr%C3%AAte%20bay%C3%A9sienne/images/image1.png)

L'un des types de r√©gression bay√©sienne les plus utiles est la r√©gression bay√©sienne de cr√™te qui estime un mod√®le probabiliste du probl√®me de r√©gression. Ici, l'ant√©riorit√© du coefficient w est donn√©e par une gaussienne sph√©rique, comme suit -

![mod√®le math√©matique de r√©gression bay√©sienne de cr√™te](https://raw.githubusercontent.com/Microleadoff/content/master/lang/fr/courses/Framework%20%26%20Librairies/Scikit%20Learn/0090%20-%20Mod%C3%A8le%20lin%C3%A9aire%20%3A%20R%C3%A9gression%20en%20cr%C3%AAte%20bay%C3%A9sienne/images/image2.png)

Ce mod√®le r√©sultant est appel√© Bayesian Ridge Regression et dans scikit-learn **sklearn.linear_model.BeyesianRidge**  est le module utilis√©.

## Param√®tres

La liste suivante pr√©sente les param√®tres utilis√©s par le module BayesianRidge :

- **n_iter** - int, facultatif. Il repr√©sente le nombre maximum d'it√©rations. La valeur par d√©faut est 300 mais la valeur d√©finie par l'utilisateur doit √™tre sup√©rieure ou √©gale √† 1.
- **fit_intercept** - Bool√©en, facultatif, par d√©faut True. Il d√©cide si l'intercept doit √™tre calcul√© pour ce mod√®le ou non. Aucune interception ne sera utilis√©e dans le calcul, si elle est d√©finie √† false.
- **tol** - float, optionnel, default=1.e-3. Il repr√©sente la pr√©cision de la solution et arr√™tera l'algorithme si w a converg√©.
- **alpha_1** - float, optional, default=1.e-6. C'est le 1er hyperparam√®tre qui est un param√®tre de forme pour la distribution Gamma ant√©rieure au param√®tre alpha.
- **alpha_2** - float, optional, default=1.e-6. C'est le 2√®me hyperparam√®tre qui est un param√®tre d'√©chelle inverse pour la distribution Gamma ant√©rieure au param√®tre alpha.
- **lambda_1** - float, optional, default=1.e-6. C'est le 1er hyperparam√®tre qui est un param√®tre de forme pour la distribution Gamma ant√©rieure au param√®tre lambda.
- **lambda_2** - float, optional, default=1.e-6. Il s'agit du 2√®me hyperparam√®tre qui est un param√®tre d'√©chelle inverse pour la priorit√© de la distribution Gamma sur le param√®tre lambda.
- **copy_X** - Bool√©en, facultatif, par d√©faut = True. Par d√©faut, il est vrai, ce qui signifie que X sera copi√©. Mais s'il est d√©fini √† false, X peut √™tre √©cras√©.
- **compute_score** - bool√©en, facultatif, par d√©faut=False. S'il est d√©fini √† true, il calcule la vraisemblance marginale logarithmique √† chaque it√©ration de l'optimisation.
- **verbose** - Bool√©en, facultatif, par d√©faut=False. Par d√©faut, il est faux mais s'il est vrai, le mode verbeux sera activ√© pendant l'ajustement du mod√®le.

## Attributs

La liste suivante pr√©sente les attributs utilis√©s par le module BayesianRidge.

- **coef_** - tableau, forme = n_features. Cet attribut fournit les vecteurs de poids.
- **intercept_** - float. Il repr√©sente le terme ind√©pendant dans la fonction de d√©cision.
- **alpha_** - float. Cet attribut fournit la pr√©cision estim√©e du bruit.
- **lambda_** - float. Cet attribut fournit la pr√©cision estim√©e du poids.
- **n_iter_** - int. Il fournit le nombre r√©el d'it√©rations prises par l'algorithme pour atteindre le crit√®re d'arr√™t.
- **sigma_** - tableau, forme = (n_features, n_features). Il fournit la matrice de variance-covariance estim√©e des poids.
- **scores_** - tableau, forme = (n_iter_+1). Il fournit la valeur de la vraisemblance marginale logarithmique √† chaque it√©ration de l'optimisation. Dans le score r√©sultant, le tableau commence par la valeur de la vraisemblance marginale logarithmique obtenue pour les valeurs initiales de a et Œª, et se termine par la valeur obtenue pour a et Œª estim√©s.

## Exemple d'impl√©mentation

Le script Python suivant fournit un exemple simple d'ajustement d'un mod√®le de r√©gression bay√©sienne de cr√™te en utilisant le module BayesianRidge de sklearn.

```python
from sklearn import linear_model
X = [[0, 0], [1, 1], [2, 2], [3, 3]]
Y = [0, 1, 2, 3]
BayReg = linear_model.BayesianRidge()
BayReg.fit(X, Y)
```

## Sortie

```python
BayesianRidge(alpha_1 = 1e-06, alpha_2 = 1e-06, compute_score = False, copy_X = True,
    fit_intercept = True, lambda_1 = 1e-06, lambda_2 = 1e-06, n_iter = 300,
    normalize = False, tol=0.001, verbose = False)
```

A partir de la sortie ci-dessus, nous pouvons v√©rifier les param√®tres du mod√®le utilis√©s dans le calcul.

## Exemple

Maintenant, une fois ajust√©, le mod√®le peut pr√©dire de nouvelles valeurs comme suit :

```python
BayReg.predict([[1,1]])
```

## Sortie

```python
array([1.00000007])
```

## Exemple

De m√™me, nous pouvons acc√©der au coefficient w du mod√®le de la mani√®re suivante :

```python
BayReg.coef_
```

## Sortie

```python
array([0.49999993, 0.49999993])
```